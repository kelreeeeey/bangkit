{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kelreeeeey/bangkit/blob/main/Submission%20A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "521026dd",
      "metadata": {
        "id": "521026dd"
      },
      "source": [
        "# PROBLEM A1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "446ce550",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "446ce550",
        "outputId": "f45a606c-496b-412c-ed3a-72ea9ea03cd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 145.0459\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 144.7791\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 144.5133\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 144.2482\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 143.9839\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 143.7205\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 143.4578\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 143.1960\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 142.9350\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 142.6747\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 142.4153\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 142.1567\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 141.8988\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 141.6418\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 141.3855\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 141.1301\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 140.8754\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 140.6215\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 140.3684\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 140.1160\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 139.8644\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 139.6136\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 139.3636\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 139.1143\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 138.8658\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 138.6180\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 138.3710\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 138.1248\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 137.8793\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 137.6346\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 137.3906\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 137.1474\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 136.9048\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 136.6631\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 136.4220\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 136.1818\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 135.9422\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 135.7034\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 135.4653\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 135.2279\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 134.9913\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 134.7553\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 134.5201\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 134.2856\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 134.0518\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 133.8188\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 133.5864\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 133.3547\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 133.1238\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 132.8935\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 132.6640\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 132.4351\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 132.2069\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 131.9795\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 131.7527\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 131.5266\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 131.3012\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 131.0764\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 130.8524\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 130.6290\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 130.4063\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 130.1843\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 129.9630\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 129.7423\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 129.5222\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 129.3029\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 129.0842\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 128.8662\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 128.6488\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 128.4321\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 128.2160\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 128.0006\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 127.7858\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 127.5717\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 127.3582\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 127.1454\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 126.9332\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 126.7217\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 126.5107\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 126.3005\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 126.0908\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 125.8818\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 125.6734\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 125.4656\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 125.2585\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 125.0519\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 124.8460\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 124.6407\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 124.4361\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 124.2320\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 124.0286\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 123.8257\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 123.6235\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 123.4218\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 123.2208\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 123.0204\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 122.8206\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 122.6213\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 122.4227\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 122.2246\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 122.0272\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 121.8303\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 121.6340\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 121.4383\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 121.2432\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 121.0487\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 120.8547\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 120.6614\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 120.4686\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 120.2763\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 120.0847\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 119.8936\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 119.7031\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 119.5131\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 119.3237\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 119.1349\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 118.9466\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 118.7589\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 118.5717\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 118.3851\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 118.1991\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 118.0136\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 117.8286\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 117.6442\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 117.4604\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 117.2770\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 117.0943\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 116.9120\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 116.7303\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 116.5491\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 116.3685\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 116.1884\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 116.0089\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 115.8298\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 115.6513\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 115.4733\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 115.2959\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 115.1189\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 114.9425\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 114.7666\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 114.5912\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 114.4163\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 114.2420\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 114.0681\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 113.8948\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 113.7219\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 113.5496\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 113.3778\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 113.2065\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 113.0356\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 112.8653\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 112.6955\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 112.5262\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 112.3573\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 112.1890\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 112.0211\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 111.8538\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 111.6869\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 111.5205\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 111.3546\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 111.1892\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 111.0243\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 110.8598\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 110.6959\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 110.5323\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 110.3693\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 110.2068\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 110.0447\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 109.8831\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 109.7219\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 109.5613\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 109.4010\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 109.2413\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 109.0820\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 108.9232\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 108.7648\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 108.6069\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 108.4495\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 108.2925\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 108.1359\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 107.9798\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 107.8242\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 107.6690\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 107.5142\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 107.3599\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 107.2061\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 107.0527\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 106.8997\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 106.7472\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 106.5951\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 106.4434\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 106.2922\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 106.1414\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 105.9911\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 105.8411\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 105.6916\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 105.5426\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 105.3939\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 105.2457\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 105.0979\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 104.9505\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 104.8036\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 104.6571\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 104.5110\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 104.3653\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 104.2200\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 104.0751\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 103.9306\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 103.7866\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 103.6430\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 103.4998\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 103.3569\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 103.2145\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 103.0725\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 102.9309\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 102.7897\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 102.6489\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 102.5085\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 102.3685\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 102.2289\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 102.0897\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 101.9509\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 101.8125\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 101.6744\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 101.5368\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 101.3995\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 101.2626\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 101.1262\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 100.9901\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 100.8544\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 100.7190\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 100.5841\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 100.4495\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 100.3153\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 100.1815\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 100.0480\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 99.9150\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 99.7823\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 99.6500\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 99.5180\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 99.3864\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 99.2552\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 99.1244\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 98.9939\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 98.8638\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 98.7340\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 98.6046\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 98.4756\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 98.3469\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 98.2186\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 98.0907\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 97.9631\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 97.8359\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 97.7090\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 97.5824\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 97.4563\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 97.3304\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 97.2049\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 97.0798\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 96.9550\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 96.8306\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 96.7065\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 96.5827\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 96.4593\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 96.3363\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 96.2135\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 96.0911\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 95.9691\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 95.8474\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 95.7260\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 95.6049\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 95.4842\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 95.3638\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 95.2438\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 95.1241\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 95.0047\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 94.8856\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 94.7669\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 94.6485\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 94.5304\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 94.4126\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 94.2952\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 94.1781\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 94.0613\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 93.9448\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 93.8286\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 93.7128\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 93.5973\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 93.4821\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 93.3672\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 93.2526\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 93.1383\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 93.0244\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 92.9107\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 92.7974\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 92.6843\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 92.5716\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 92.4592\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 92.3471\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 92.2352\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 92.1237\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 92.0125\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 91.9016\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 91.7910\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 91.6807\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 91.5707\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 91.4610\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 91.3515\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 91.2424\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 91.1336\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 91.0251\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 90.9168\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 90.8089\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 90.7012\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 90.5938\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 90.4867\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 90.3799\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 90.2734\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 90.1672\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 90.0612\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 89.9556\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 89.8502\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 89.7451\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 89.6403\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 89.5358\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 89.4315\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 89.3275\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 89.2238\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 89.1204\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 89.0172\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 88.9144\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 88.8118\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 88.7094\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 88.6074\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 88.5056\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 88.4040\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 88.3028\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 88.2018\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 88.1011\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 88.0006\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 87.9005\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 87.8005\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 87.7009\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 87.6015\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 87.5023\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 87.4035\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 87.3049\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 87.2065\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 87.1084\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 87.0106\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 86.9130\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 86.8157\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 86.7186\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 86.6218\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 86.5252\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 86.4289\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 86.3328\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 86.2370\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 86.1415\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 86.0462\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 85.9511\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 85.8563\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 85.7617\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 85.6674\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 85.5733\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 85.4795\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 85.3859\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 85.2926\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 85.1995\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 85.1066\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 85.0140\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 84.9216\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 84.8294\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 84.7375\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 84.6459\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 84.5544\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 84.4632\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 84.3723\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 84.2815\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 84.1911\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 84.1008\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 84.0108\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 83.9210\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 83.8314\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 83.7420\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 83.6529\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 83.5640\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 83.4754\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 83.3870\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 83.2988\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 83.2108\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 83.1230\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 83.0355\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 82.9482\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 82.8611\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 82.7742\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 82.6876\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 82.6012\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 82.5149\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 82.4290\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 82.3432\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 82.2576\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 82.1723\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 82.0872\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 82.0023\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 81.9176\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 81.8331\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 81.7488\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 81.6648\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 81.5809\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 81.4973\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 81.4139\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 81.3307\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 81.2477\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 81.1649\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 81.0823\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 80.9999\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 80.9177\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 80.8357\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 80.7540\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 80.6724\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 80.5910\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 80.5099\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 80.4289\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 80.3482\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 80.2676\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 80.1873\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 80.1071\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 80.0272\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 79.9474\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 79.8678\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 79.7885\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 79.7093\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 79.6303\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 79.5516\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 79.4730\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 79.3946\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 79.3164\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 79.2384\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 79.1606\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 79.0829\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 79.0055\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 78.9283\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 78.8512\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 78.7743\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 78.6977\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 78.6212\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 78.5449\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 78.4688\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 78.3928\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 78.3171\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 78.2415\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 78.1661\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 78.0909\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 78.0159\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 77.9411\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 77.8664\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 77.7920\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 77.7177\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 77.6436\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 77.5696\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 77.4959\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 77.4223\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 77.3489\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 77.2757\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 77.2026\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 77.1298\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 77.0571\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 76.9845\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 76.9122\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 76.8400\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 76.7680\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 76.6962\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 76.6245\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 76.5531\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 76.4817\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 76.4106\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 76.3396\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 76.2688\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 76.1982\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 76.1277\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 76.0574\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 75.9873\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 75.9173\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 75.8475\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 75.7778\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 75.7083\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 75.6390\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 75.5699\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 75.5009\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 75.4321\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 75.3634\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 75.2949\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 75.2265\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 75.1584\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 75.0903\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 75.0225\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 74.9548\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 74.8872\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 74.8198\n",
            "[[0.28612113]\n",
            " [4.0936165 ]]\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# PROBLEM A1\n",
        "#\n",
        "# Given two arrays, train a neural network model to match the X to the Y.\n",
        "# Predict the model with new values of X [-2.0, 10.0]\n",
        "# We provide the model prediction, do not change the code.\n",
        "#\n",
        "# The test infrastructure expects a trained model that accepts\n",
        "# an input shape of [1].\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# Please be aware that this is a linear model.\n",
        "# We will test your model with values in a range as defined in the array to make sure your model is linear.\n",
        "#\n",
        "# Desired loss (MSE) < 1e-4\n",
        "# =================================================================================\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "def solution_A1():\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    X = np.array([-4.0, -3.0, -2.0, -1.0, 0.0, 1.0,\n",
        "                 2.0, 3.0, 4.0, 5.0], dtype=float)\n",
        "    Y = np.array([5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0,\n",
        "                 12.0, 13.0, 14.0, ], dtype=float)\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Dense(units=1, input_shape=[1]),\n",
        "    ])\n",
        "\n",
        "    opt = keras.optimizers.SGD(learning_rate=0.0001)\n",
        "\n",
        "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
        "\n",
        "    model.fit(X, Y, epochs=500)\n",
        "    print(model.predict([-2.0, 10.0]))\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_A1()\n",
        "    model.save(\"model_A1.h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "010b6ac3",
      "metadata": {
        "id": "010b6ac3"
      },
      "source": [
        "# PROBLEM A2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "61a0605a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61a0605a",
        "outputId": "4f0fca2a-2bf9-41de-ff74-590c91330ca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n",
            "Epoch 1/50\n",
            "33/33 - 14s - loss: 0.9765 - accuracy: 0.5219 - val_loss: 0.7402 - val_accuracy: 0.5000 - 14s/epoch - 417ms/step\n",
            "Epoch 2/50\n",
            "33/33 - 13s - loss: 0.6470 - accuracy: 0.6241 - val_loss: 0.6880 - val_accuracy: 0.5391 - 13s/epoch - 389ms/step\n",
            "Epoch 3/50\n",
            "33/33 - 12s - loss: 0.5707 - accuracy: 0.7186 - val_loss: 1.4675 - val_accuracy: 0.5000 - 12s/epoch - 374ms/step\n",
            "Epoch 4/50\n",
            "33/33 - 12s - loss: 0.4522 - accuracy: 0.7838 - val_loss: 2.5443 - val_accuracy: 0.5000 - 12s/epoch - 375ms/step\n",
            "Epoch 5/50\n",
            "33/33 - 13s - loss: 0.4186 - accuracy: 0.8014 - val_loss: 0.8410 - val_accuracy: 0.5820 - 13s/epoch - 392ms/step\n",
            "Epoch 6/50\n",
            "33/33 - 12s - loss: 0.3799 - accuracy: 0.8345 - val_loss: 2.0847 - val_accuracy: 0.5156 - 12s/epoch - 374ms/step\n",
            "Epoch 7/50\n",
            "33/33 - 12s - loss: 0.3711 - accuracy: 0.8598 - val_loss: 1.2133 - val_accuracy: 0.6016 - 12s/epoch - 373ms/step\n",
            "Epoch 8/50\n",
            "33/33 - 12s - loss: 0.2682 - accuracy: 0.8880 - val_loss: 0.9054 - val_accuracy: 0.6133 - 12s/epoch - 375ms/step\n",
            "Epoch 9/50\n",
            "33/33 - 12s - loss: 0.2587 - accuracy: 0.8783 - val_loss: 0.5528 - val_accuracy: 0.7461 - 12s/epoch - 374ms/step\n",
            "Epoch 10/50\n",
            "33/33 - 12s - loss: 0.1872 - accuracy: 0.9270 - val_loss: 2.7210 - val_accuracy: 0.5195 - 12s/epoch - 373ms/step\n",
            "Epoch 11/50\n",
            "33/33 - 12s - loss: 0.2043 - accuracy: 0.9250 - val_loss: 0.9855 - val_accuracy: 0.7734 - 12s/epoch - 374ms/step\n",
            "Epoch 12/50\n",
            "33/33 - 12s - loss: 0.2025 - accuracy: 0.9270 - val_loss: 1.1326 - val_accuracy: 0.6680 - 12s/epoch - 373ms/step\n",
            "Epoch 13/50\n",
            "33/33 - 12s - loss: 0.1526 - accuracy: 0.9396 - val_loss: 3.2182 - val_accuracy: 0.5312 - 12s/epoch - 374ms/step\n",
            "Epoch 14/50\n",
            "33/33 - 12s - loss: 0.1411 - accuracy: 0.9474 - val_loss: 2.1623 - val_accuracy: 0.5625 - 12s/epoch - 370ms/step\n",
            "Epoch 15/50\n",
            "33/33 - 12s - loss: 0.3216 - accuracy: 0.9396 - val_loss: 1.8130 - val_accuracy: 0.6055 - 12s/epoch - 376ms/step\n",
            "Epoch 16/50\n",
            "33/33 - 12s - loss: 0.1213 - accuracy: 0.9611 - val_loss: 1.2951 - val_accuracy: 0.7070 - 12s/epoch - 375ms/step\n",
            "Epoch 17/50\n",
            "33/33 - 12s - loss: 0.0919 - accuracy: 0.9620 - val_loss: 1.7057 - val_accuracy: 0.6875 - 12s/epoch - 375ms/step\n",
            "Epoch 18/50\n",
            "33/33 - 13s - loss: 0.0864 - accuracy: 0.9776 - val_loss: 2.0096 - val_accuracy: 0.5820 - 13s/epoch - 404ms/step\n",
            "Epoch 19/50\n",
            "33/33 - 12s - loss: 0.1657 - accuracy: 0.9416 - val_loss: 1.8138 - val_accuracy: 0.6641 - 12s/epoch - 378ms/step\n",
            "Epoch 20/50\n",
            "33/33 - 12s - loss: 0.1023 - accuracy: 0.9591 - val_loss: 0.7747 - val_accuracy: 0.7891 - 12s/epoch - 374ms/step\n",
            "Epoch 21/50\n",
            "33/33 - 12s - loss: 0.0684 - accuracy: 0.9815 - val_loss: 0.4390 - val_accuracy: 0.8750 - 12s/epoch - 371ms/step\n",
            "Epoch 22/50\n",
            "33/33 - 12s - loss: 0.0891 - accuracy: 0.9737 - val_loss: 2.5264 - val_accuracy: 0.6641 - 12s/epoch - 375ms/step\n",
            "Epoch 23/50\n",
            "33/33 - 12s - loss: 0.0945 - accuracy: 0.9708 - val_loss: 0.8506 - val_accuracy: 0.8047 - 12s/epoch - 374ms/step\n",
            "Epoch 24/50\n",
            "33/33 - 12s - loss: 0.1319 - accuracy: 0.9649 - val_loss: 0.8075 - val_accuracy: 0.8008 - 12s/epoch - 373ms/step\n",
            "Epoch 25/50\n",
            "33/33 - 12s - loss: 0.0639 - accuracy: 0.9766 - val_loss: 1.9090 - val_accuracy: 0.6875 - 12s/epoch - 377ms/step\n",
            "Epoch 26/50\n",
            "33/33 - 12s - loss: 0.1009 - accuracy: 0.9708 - val_loss: 1.5483 - val_accuracy: 0.7148 - 12s/epoch - 376ms/step\n",
            "Epoch 27/50\n",
            "33/33 - 12s - loss: 0.0872 - accuracy: 0.9786 - val_loss: 3.7663 - val_accuracy: 0.5898 - 12s/epoch - 375ms/step\n",
            "Epoch 28/50\n",
            "33/33 - 12s - loss: 0.0519 - accuracy: 0.9766 - val_loss: 2.9522 - val_accuracy: 0.6719 - 12s/epoch - 373ms/step\n",
            "Epoch 29/50\n",
            "33/33 - 12s - loss: 0.1300 - accuracy: 0.9630 - val_loss: 1.5666 - val_accuracy: 0.7344 - 12s/epoch - 371ms/step\n",
            "Epoch 30/50\n",
            "33/33 - 12s - loss: 0.0394 - accuracy: 0.9873 - val_loss: 1.9814 - val_accuracy: 0.7461 - 12s/epoch - 372ms/step\n",
            "Epoch 31/50\n",
            "33/33 - 12s - loss: 0.1239 - accuracy: 0.9630 - val_loss: 2.6766 - val_accuracy: 0.6602 - 12s/epoch - 370ms/step\n",
            "Epoch 32/50\n",
            "33/33 - 12s - loss: 0.0489 - accuracy: 0.9825 - val_loss: 2.0742 - val_accuracy: 0.6758 - 12s/epoch - 371ms/step\n",
            "Epoch 33/50\n",
            "33/33 - 12s - loss: 0.0913 - accuracy: 0.9688 - val_loss: 0.5347 - val_accuracy: 0.8281 - 12s/epoch - 368ms/step\n",
            "Epoch 34/50\n",
            "33/33 - 13s - loss: 0.0503 - accuracy: 0.9776 - val_loss: 1.1645 - val_accuracy: 0.7812 - 13s/epoch - 392ms/step\n",
            "Epoch 35/50\n",
            "33/33 - 12s - loss: 0.1339 - accuracy: 0.9611 - val_loss: 1.0039 - val_accuracy: 0.7734 - 12s/epoch - 368ms/step\n",
            "Epoch 36/50\n",
            "33/33 - 12s - loss: 0.0512 - accuracy: 0.9796 - val_loss: 3.2911 - val_accuracy: 0.6328 - 12s/epoch - 368ms/step\n",
            "Epoch 37/50\n",
            "33/33 - 12s - loss: 0.0960 - accuracy: 0.9698 - val_loss: 0.9335 - val_accuracy: 0.8242 - 12s/epoch - 370ms/step\n",
            "Epoch 38/50\n",
            "33/33 - 12s - loss: 0.0389 - accuracy: 0.9834 - val_loss: 3.5811 - val_accuracy: 0.6523 - 12s/epoch - 370ms/step\n",
            "Epoch 39/50\n",
            "33/33 - 12s - loss: 0.1495 - accuracy: 0.9688 - val_loss: 2.1064 - val_accuracy: 0.7422 - 12s/epoch - 367ms/step\n",
            "Epoch 40/50\n",
            "33/33 - 12s - loss: 0.0639 - accuracy: 0.9844 - val_loss: 4.6047 - val_accuracy: 0.5664 - 12s/epoch - 368ms/step\n",
            "Epoch 41/50\n",
            "33/33 - 12s - loss: 0.0576 - accuracy: 0.9854 - val_loss: 1.1497 - val_accuracy: 0.7852 - 12s/epoch - 370ms/step\n",
            "Epoch 42/50\n",
            "33/33 - 12s - loss: 0.1058 - accuracy: 0.9786 - val_loss: 1.0841 - val_accuracy: 0.7969 - 12s/epoch - 371ms/step\n",
            "Epoch 43/50\n",
            "33/33 - 12s - loss: 0.0487 - accuracy: 0.9854 - val_loss: 0.7890 - val_accuracy: 0.8398 - 12s/epoch - 369ms/step\n",
            "Epoch 44/50\n",
            "33/33 - 12s - loss: 0.0590 - accuracy: 0.9805 - val_loss: 2.1464 - val_accuracy: 0.7500 - 12s/epoch - 370ms/step\n",
            "Epoch 45/50\n",
            "33/33 - 12s - loss: 0.0340 - accuracy: 0.9922 - val_loss: 1.0273 - val_accuracy: 0.8203 - 12s/epoch - 368ms/step\n",
            "Epoch 46/50\n",
            "33/33 - 12s - loss: 0.0405 - accuracy: 0.9834 - val_loss: 1.6861 - val_accuracy: 0.7773 - 12s/epoch - 362ms/step\n",
            "Epoch 47/50\n",
            "33/33 - 12s - loss: 0.0770 - accuracy: 0.9766 - val_loss: 4.9573 - val_accuracy: 0.5898 - 12s/epoch - 369ms/step\n",
            "Epoch 48/50\n",
            "33/33 - 12s - loss: 0.0482 - accuracy: 0.9805 - val_loss: 0.9791 - val_accuracy: 0.8281 - 12s/epoch - 365ms/step\n",
            "Epoch 49/50\n",
            "33/33 - 13s - loss: 0.0275 - accuracy: 0.9903 - val_loss: 1.7883 - val_accuracy: 0.7539 - 13s/epoch - 389ms/step\n",
            "Epoch 50/50\n",
            "33/33 - 12s - loss: 0.0490 - accuracy: 0.9854 - val_loss: 0.5728 - val_accuracy: 0.9062 - 12s/epoch - 367ms/step\n",
            "<keras.callbacks.History object at 0x7f5726480810>\n"
          ]
        }
      ],
      "source": [
        "# =====================================================================================\n",
        "# PROBLEM A2\n",
        "#\n",
        "# Build a Neural Network Model for Horse or Human Dataset.\n",
        "# The test will expect it to classify binary classes.\n",
        "# Your input layer should accept 150x150 with 3 bytes color as the input shape.\n",
        "# Don't use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 83%\n",
        "# ======================================================================================\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "def solution_A2():\n",
        "    data_url_1 = 'https://github.com/dicodingacademy/assets/releases/download/release-horse-or-human/horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_1, 'horse-or-human.zip')\n",
        "    local_file = 'horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/horse-or-human')\n",
        "\n",
        "    data_url_2 = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/validation-horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_2, 'validation-horse-or-human.zip')\n",
        "    local_file = 'validation-horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/validation-horse-or-human')\n",
        "    zip_ref.close()\n",
        "\n",
        "    TRAINING_DIR = 'data/horse-or-human'\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        # YOUR CODE HERE\n",
        "        rescale=1.0 / 255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # YOUR IMAGE SIZE SHOULD BE 150x150\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        directory=TRAINING_DIR,\n",
        "        batch_size=32,\n",
        "        class_mode='binary',\n",
        "        target_size=(150, 150)\n",
        "    )  # YOUR CODE HERE\n",
        "\n",
        "    VALIDATION_DIR = 'data/validation-horse-or-human'\n",
        "    validation_datagen = ImageDataGenerator(\n",
        "        # YOUR CODE HERE\n",
        "        rescale=1.0 / 255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # YOUR IMAGE SIZE SHOULD BE 150x150\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        directory=VALIDATION_DIR,\n",
        "        batch_size=100,\n",
        "        class_mode='binary',\n",
        "        target_size=(150, 150)\n",
        "    )  # YOUR CODE HERE\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # YOUR CODE HERE, end with a Neuron Dense, activated by sigmoid\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(320, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=RMSprop(learning_rate=0.001,\n",
        "                  rho=0.9, momentum=0.4, epsilon=1e-07, centered=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(train_generator,\n",
        "                        epochs=50,\n",
        "                        verbose=2,\n",
        "                        validation_data=validation_generator)\n",
        "    print(history)\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_A2()\n",
        "    model.save(\"model_A2.h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROBLEM A3"
      ],
      "metadata": {
        "id": "r2lOar61A0FH"
      },
      "id": "r2lOar61A0FH"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "514ab4d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "514ab4d1",
        "outputId": "9bf02d23-1a2b-4e4b-b9eb-d95602e42332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n",
            "Epoch 1/15\n",
            "33/33 - 18s - loss: 0.8758 - accuracy: 0.8939 - val_loss: 0.0034 - val_accuracy: 1.0000 - 18s/epoch - 549ms/step\n",
            "Epoch 2/15\n",
            "33/33 - 12s - loss: 0.1690 - accuracy: 0.9601 - val_loss: 0.0059 - val_accuracy: 0.9961 - 12s/epoch - 356ms/step\n",
            "Epoch 3/15\n",
            "33/33 - 12s - loss: 0.1294 - accuracy: 0.9640 - val_loss: 0.0106 - val_accuracy: 0.9961 - 12s/epoch - 356ms/step\n",
            "Epoch 4/15\n",
            "33/33 - 12s - loss: 0.1224 - accuracy: 0.9679 - val_loss: 0.0207 - val_accuracy: 0.9922 - 12s/epoch - 357ms/step\n",
            "Epoch 5/15\n",
            "33/33 - 12s - loss: 0.1526 - accuracy: 0.9620 - val_loss: 5.5971e-04 - val_accuracy: 1.0000 - 12s/epoch - 356ms/step\n",
            "Epoch 6/15\n",
            "33/33 - 12s - loss: 0.0765 - accuracy: 0.9757 - val_loss: 0.0022 - val_accuracy: 1.0000 - 12s/epoch - 366ms/step\n",
            "Epoch 7/15\n",
            "33/33 - 12s - loss: 0.0975 - accuracy: 0.9766 - val_loss: 5.6724e-05 - val_accuracy: 1.0000 - 12s/epoch - 355ms/step\n",
            "Epoch 8/15\n",
            "33/33 - 12s - loss: 0.0741 - accuracy: 0.9834 - val_loss: 0.0152 - val_accuracy: 0.9922 - 12s/epoch - 352ms/step\n",
            "Epoch 9/15\n",
            "33/33 - 12s - loss: 0.0940 - accuracy: 0.9805 - val_loss: 2.6792e-05 - val_accuracy: 1.0000 - 12s/epoch - 353ms/step\n",
            "Epoch 10/15\n",
            "33/33 - 12s - loss: 0.0510 - accuracy: 0.9922 - val_loss: 5.5284e-04 - val_accuracy: 1.0000 - 12s/epoch - 357ms/step\n",
            "Epoch 11/15\n",
            "33/33 - 12s - loss: 0.0391 - accuracy: 0.9932 - val_loss: 0.3929 - val_accuracy: 0.9141 - 12s/epoch - 377ms/step\n",
            "Epoch 12/15\n",
            "33/33 - 11s - loss: 0.0804 - accuracy: 0.9893 - val_loss: 4.6709e-04 - val_accuracy: 1.0000 - 11s/epoch - 347ms/step\n",
            "Epoch 13/15\n",
            "33/33 - 12s - loss: 0.0576 - accuracy: 0.9883 - val_loss: 0.2737 - val_accuracy: 0.9375 - 12s/epoch - 350ms/step\n",
            "Epoch 14/15\n",
            "33/33 - 12s - loss: 0.0786 - accuracy: 0.9844 - val_loss: 4.1469e-05 - val_accuracy: 1.0000 - 12s/epoch - 355ms/step\n",
            "Epoch 15/15\n",
            "33/33 - 12s - loss: 0.0395 - accuracy: 0.9912 - val_loss: 0.0304 - val_accuracy: 0.9844 - 12s/epoch - 357ms/step\n",
            "<keras.callbacks.History object at 0x7f56ffee03d0>\n"
          ]
        }
      ],
      "source": [
        "# ======================================================================================================\n",
        "# PROBLEM A3\n",
        "#\n",
        "# Build a classifier for the Human or Horse Dataset with Transfer Learning.\n",
        "# The test will expect it to classify binary classes.\n",
        "# Note that all the layers in the pre-trained model are non-trainable.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The horse-or-human dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n",
        "# Inception_v3, pre-trained model used in this problem is developed by Google.\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 97%.\n",
        "# =======================================================================================================\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "\n",
        "def solution_A3():\n",
        "    inceptionv3 = 'https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "    urllib.request.urlretrieve(\n",
        "        inceptionv3, 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "    local_weights_file = 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "    pre_trained_model = InceptionV3(\n",
        "        input_shape=(150, 150, 3),\n",
        "        include_top=False,\n",
        "        weights=None\n",
        "    )\n",
        "\n",
        "    pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "    for layer in pre_trained_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    last_layer = pre_trained_model.get_layer('mixed10').output\n",
        "\n",
        "    data_url_1 = 'https://github.com/dicodingacademy/assets/releases/download/release-horse-or-human/horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_1, 'horse-or-human.zip')\n",
        "    local_file = 'horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/horse-or-human')\n",
        "    zip_ref.close()\n",
        "\n",
        "    data_url_2 = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/validation-horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_2, 'validation-horse-or-human.zip')\n",
        "    local_file = 'validation-horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('data/validation-horse-or-human')\n",
        "    zip_ref.close()\n",
        "\n",
        "    train_dir = 'data/horse-or-human'\n",
        "    validation_dir = 'data/validation-horse-or-human'\n",
        "\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        # YOUR CODE HERE\n",
        "        rescale=1.0 / 255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # YOUR IMAGE SIZE SHOULD BE 150x150\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        directory=train_dir,\n",
        "        batch_size=32,\n",
        "        class_mode='binary',\n",
        "        target_size=(150, 150)\n",
        "    )  # YOUR CODE HERE\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(\n",
        "        # YOUR CODE HERE\n",
        "        rescale=1.0 / 255,\n",
        "    )\n",
        "\n",
        "    # YOUR IMAGE SIZE SHOULD BE 150x150\n",
        "    validation_generator = validation_datagen.flow_from_directory(\n",
        "        directory=validation_dir,\n",
        "        batch_size=64,\n",
        "        class_mode='binary',\n",
        "        target_size=(150, 150)\n",
        "    )  # YOUR CODE HERE\n",
        "\n",
        "    x = layers.Flatten()(last_layer)  # YOUR CODE HERE, BUT END WITH A Neuron Dense, activated by sigmoid\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(pre_trained_model.input, x)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=RMSprop(learning_rate=0.0001,\n",
        "                  rho=0.9, momentum=0.0, epsilon=1e-07, centered=False),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    history = model.fit(train_generator,\n",
        "                        epochs = 15,\n",
        "                        verbose = 2,\n",
        "                        callbacks=None,\n",
        "                        validation_split=0.0,\n",
        "                        validation_data = validation_generator)\n",
        "    print(history)\n",
        "\n",
        "    return model\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_A3()\n",
        "    model.save(\"model_A3.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================================================================\n",
        "# PROBLEM A4\n",
        "#\n",
        "# Build and train a binary classifier for the IMDB review dataset.\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is originally published in http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 83%\n",
        "# ===========================================================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "def solution_A4():\n",
        "    imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
        "    # YOUR CODE HERE\n",
        "    train_data, test_data = imdb['train'], imdb['test']\n",
        "\n",
        "    training_sentences = []\n",
        "    training_labels = []\n",
        "\n",
        "    testing_sentences = []\n",
        "    testing_labels = []\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    for s, l in train_data:\n",
        "        training_sentences.append(s.numpy().decode('utf8'))\n",
        "        training_labels.append(l.numpy())\n",
        "\n",
        "    for s, l in test_data:\n",
        "        testing_sentences.append(s.numpy().decode('utf8'))\n",
        "        testing_labels.append(l.numpy())\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    training_labels_final = np.array(training_labels)\n",
        "    testing_labels_final = np.array(testing_labels)\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # Make sure you used all of these parameters or test may fail\n",
        "    vocab_size = 10000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type = 'post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "\n",
        "    # Fit your tokenizer with training data\n",
        "    tokenizer =  Tokenizer(num_words= vocab_size)\n",
        "    tokenizer.fit_on_texts(training_sentences)\n",
        "    word_index = tokenizer.word_index\n",
        "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "    training_padded = pad_sequences(training_sequences, maxlen=max_length, truncating=trunc_type)\n",
        "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "    testing_padded = pad_sequences(testing_sequences, maxlen=max_length)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        # YOUR CODE HERE. Do not change the last layer.\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(6, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=Adam(learning_rate=0.0001),\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    history = model.fit(training_padded, training_labels_final,\n",
        "                        epochs = 15,\n",
        "                        verbose = 2,\n",
        "                        callbacks=None,\n",
        "                        validation_split=0.0,\n",
        "                        validation_data = (testing_padded, testing_labels_final))\n",
        "    print(history)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model = solution_A4()\n",
        "    model.save(\"model_A4.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN2wQKgdPvpE",
        "outputId": "501383f0-8da1-476e-929e-cad1ad204496"
      },
      "id": "oN2wQKgdPvpE",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "782/782 - 4s - loss: 0.6925 - accuracy: 0.5138 - val_loss: 0.6915 - val_accuracy: 0.5472 - 4s/epoch - 6ms/step\n",
            "Epoch 2/15\n",
            "782/782 - 3s - loss: 0.6780 - accuracy: 0.6512 - val_loss: 0.6538 - val_accuracy: 0.7138 - 3s/epoch - 4ms/step\n",
            "Epoch 3/15\n",
            "782/782 - 3s - loss: 0.5754 - accuracy: 0.7701 - val_loss: 0.5039 - val_accuracy: 0.8029 - 3s/epoch - 4ms/step\n",
            "Epoch 4/15\n",
            "782/782 - 5s - loss: 0.4339 - accuracy: 0.8329 - val_loss: 0.4054 - val_accuracy: 0.8351 - 5s/epoch - 6ms/step\n",
            "Epoch 5/15\n",
            "782/782 - 5s - loss: 0.3545 - accuracy: 0.8647 - val_loss: 0.3631 - val_accuracy: 0.8480 - 5s/epoch - 6ms/step\n",
            "Epoch 6/15\n",
            "782/782 - 3s - loss: 0.3083 - accuracy: 0.8816 - val_loss: 0.3409 - val_accuracy: 0.8552 - 3s/epoch - 4ms/step\n",
            "Epoch 7/15\n",
            "782/782 - 3s - loss: 0.2749 - accuracy: 0.8969 - val_loss: 0.3316 - val_accuracy: 0.8580 - 3s/epoch - 4ms/step\n",
            "Epoch 8/15\n",
            "782/782 - 3s - loss: 0.2477 - accuracy: 0.9082 - val_loss: 0.3248 - val_accuracy: 0.8596 - 3s/epoch - 4ms/step\n",
            "Epoch 9/15\n",
            "782/782 - 3s - loss: 0.2239 - accuracy: 0.9191 - val_loss: 0.3231 - val_accuracy: 0.8591 - 3s/epoch - 4ms/step\n",
            "Epoch 10/15\n",
            "782/782 - 3s - loss: 0.2021 - accuracy: 0.9303 - val_loss: 0.3266 - val_accuracy: 0.8582 - 3s/epoch - 4ms/step\n",
            "Epoch 11/15\n",
            "782/782 - 3s - loss: 0.1820 - accuracy: 0.9405 - val_loss: 0.3282 - val_accuracy: 0.8580 - 3s/epoch - 4ms/step\n",
            "Epoch 12/15\n",
            "782/782 - 3s - loss: 0.1627 - accuracy: 0.9497 - val_loss: 0.3332 - val_accuracy: 0.8560 - 3s/epoch - 4ms/step\n",
            "Epoch 13/15\n",
            "782/782 - 3s - loss: 0.1448 - accuracy: 0.9585 - val_loss: 0.3395 - val_accuracy: 0.8545 - 3s/epoch - 4ms/step\n",
            "Epoch 14/15\n",
            "782/782 - 3s - loss: 0.1273 - accuracy: 0.9662 - val_loss: 0.3476 - val_accuracy: 0.8529 - 3s/epoch - 4ms/step\n",
            "Epoch 15/15\n",
            "782/782 - 4s - loss: 0.1110 - accuracy: 0.9732 - val_loss: 0.3571 - val_accuracy: 0.8508 - 4s/epoch - 4ms/step\n",
            "<keras.callbacks.History object at 0x7f5473f6d510>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================================================\n",
        "# PROBLEM A5\n",
        "#\n",
        "# Build and train a neural network model using the Sunspots.csv dataset.\n",
        "# Use MAE as the metrics of your neural network model.\n",
        "# We provided code for normalizing the data. Please do not change the code.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is downloaded from kaggle.com/robervalt/sunspots\n",
        "#\n",
        "# Desired MAE < 0.15 on the normalized dataset.\n",
        "# ========================================================================================\n",
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n"
      ],
      "metadata": {
        "id": "UcIyo0_lsINt"
      },
      "id": "UcIyo0_lsINt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================================================\n",
        "# PROBLEM A5\n",
        "#\n",
        "# Build and train a neural network model using the Sunspots.csv dataset.\n",
        "# Use MAE as the metrics of your neural network model.\n",
        "# We provided code for normalizing the data. Please do not change the code.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is downloaded from kaggle.com/robervalt/sunspots\n",
        "#\n",
        "# Desired MAE < 0.15 on the normalized dataset.\n",
        "# ========================================================================================\n",
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_A5():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "    urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            sunspots.append(float(row[2]))\n",
        "            time_step.append(int(row[0]))\n",
        "\n",
        "    series = np.array(sunspots)\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    # This is the normalization function\n",
        "    min = np.min(series)\n",
        "    max = np.max(series)\n",
        "    series -= min\n",
        "    series /= max\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # The data should be split into training and validation sets at time step 3000\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    split_time = 3000\n",
        "    time_train = time[:split_time]\n",
        "    x_train = series[:split_time]\n",
        "    time_valid = time[split_time:]\n",
        "    x_valid = series[split_time:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "    train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size,\n",
        "                                 shuffle_buffer=shuffle_buffer_size)\n",
        "\n",
        "    print(np.array(train_set).shape)\n",
        "\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n",
        "                               strides=1, padding=\"causal\",\n",
        "                               activation=\"relu\",\n",
        "                               input_shape=[None, 1]),\n",
        "        tf.keras.layers.LSTM(60, return_sequences=True),\n",
        "        tf.keras.layers.LSTM(60, return_sequences=True),\n",
        "        tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='mae', optimizer='adam')\n",
        "    model.fit(train_set, batch_size=256, epochs=100)\n",
        "    # PLEASE NOTE IF YOU SEE THIS TEXT WHILE TRAINING -- IT IS SAFE TO IGNORE\n",
        "    # BaseCollectiveExecutor::StartAbort Out of range: End of sequence\n",
        "    # \t [[{{node IteratorGetNext}}]]\n",
        "    #\n",
        "\n",
        "    # YOUR CODE HERE TO COMPILE AND TRAIN THE MODEL\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_A5()\n",
        "    model.save(\"model_A5.h5\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fvh4ULKTsIKF",
        "outputId": "2b25bc4e-ab1d-4973-ce03-c716905c591e"
      },
      "id": "Fvh4ULKTsIKF",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "()\n",
            "Epoch 1/100\n",
            "93/93 [==============================] - 9s 44ms/step - loss: 0.0740\n",
            "Epoch 2/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0501\n",
            "Epoch 3/100\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 0.0467\n",
            "Epoch 4/100\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 0.0453\n",
            "Epoch 5/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0453\n",
            "Epoch 6/100\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 0.0448\n",
            "Epoch 7/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0452\n",
            "Epoch 8/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0449\n",
            "Epoch 9/100\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 0.0448\n",
            "Epoch 10/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0447\n",
            "Epoch 11/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0448\n",
            "Epoch 12/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0446\n",
            "Epoch 13/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0447\n",
            "Epoch 14/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0445\n",
            "Epoch 15/100\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 0.0444\n",
            "Epoch 16/100\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 0.0443\n",
            "Epoch 17/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0443\n",
            "Epoch 18/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0445\n",
            "Epoch 19/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0443\n",
            "Epoch 20/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0444\n",
            "Epoch 21/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0441\n",
            "Epoch 22/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0440\n",
            "Epoch 23/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0440\n",
            "Epoch 24/100\n",
            "93/93 [==============================] - 4s 44ms/step - loss: 0.0440\n",
            "Epoch 25/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0442\n",
            "Epoch 26/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0441\n",
            "Epoch 27/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0439\n",
            "Epoch 28/100\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 0.0438\n",
            "Epoch 29/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0437\n",
            "Epoch 30/100\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 0.0437\n",
            "Epoch 31/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0436\n",
            "Epoch 32/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0436\n",
            "Epoch 33/100\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 0.0434\n",
            "Epoch 34/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0436\n",
            "Epoch 35/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0433\n",
            "Epoch 36/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0434\n",
            "Epoch 37/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0433\n",
            "Epoch 38/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0433\n",
            "Epoch 39/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0432\n",
            "Epoch 40/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0430\n",
            "Epoch 41/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0430\n",
            "Epoch 42/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0430\n",
            "Epoch 43/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0432\n",
            "Epoch 44/100\n",
            "93/93 [==============================] - 6s 58ms/step - loss: 0.0429\n",
            "Epoch 45/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0428\n",
            "Epoch 46/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0427\n",
            "Epoch 47/100\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 0.0427\n",
            "Epoch 48/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0429\n",
            "Epoch 49/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0426\n",
            "Epoch 50/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0427\n",
            "Epoch 51/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0425\n",
            "Epoch 52/100\n",
            "93/93 [==============================] - 4s 46ms/step - loss: 0.0424\n",
            "Epoch 53/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0422\n",
            "Epoch 54/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0422\n",
            "Epoch 55/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0421\n",
            "Epoch 56/100\n",
            "93/93 [==============================] - 4s 44ms/step - loss: 0.0421\n",
            "Epoch 57/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0420\n",
            "Epoch 58/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0419\n",
            "Epoch 59/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0417\n",
            "Epoch 60/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0415\n",
            "Epoch 61/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0417\n",
            "Epoch 62/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0414\n",
            "Epoch 63/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0413\n",
            "Epoch 64/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0413\n",
            "Epoch 65/100\n",
            "93/93 [==============================] - 4s 44ms/step - loss: 0.0411\n",
            "Epoch 66/100\n",
            "93/93 [==============================] - 4s 44ms/step - loss: 0.0408\n",
            "Epoch 67/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0409\n",
            "Epoch 68/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0406\n",
            "Epoch 69/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0407\n",
            "Epoch 70/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0402\n",
            "Epoch 71/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0401\n",
            "Epoch 72/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0399\n",
            "Epoch 73/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0395\n",
            "Epoch 74/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0394\n",
            "Epoch 75/100\n",
            "93/93 [==============================] - 4s 44ms/step - loss: 0.0392\n",
            "Epoch 76/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0390\n",
            "Epoch 77/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0391\n",
            "Epoch 78/100\n",
            "93/93 [==============================] - 4s 44ms/step - loss: 0.0387\n",
            "Epoch 79/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0384\n",
            "Epoch 80/100\n",
            "93/93 [==============================] - 4s 44ms/step - loss: 0.0380\n",
            "Epoch 81/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0381\n",
            "Epoch 82/100\n",
            "93/93 [==============================] - 4s 44ms/step - loss: 0.0378\n",
            "Epoch 83/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0379\n",
            "Epoch 84/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0373\n",
            "Epoch 85/100\n",
            "93/93 [==============================] - 4s 42ms/step - loss: 0.0373\n",
            "Epoch 86/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0370\n",
            "Epoch 87/100\n",
            "93/93 [==============================] - 4s 44ms/step - loss: 0.0368\n",
            "Epoch 88/100\n",
            "93/93 [==============================] - 6s 59ms/step - loss: 0.0366\n",
            "Epoch 89/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0363\n",
            "Epoch 90/100\n",
            "93/93 [==============================] - 4s 44ms/step - loss: 0.0360\n",
            "Epoch 91/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0359\n",
            "Epoch 92/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0359\n",
            "Epoch 93/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0355\n",
            "Epoch 94/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0352\n",
            "Epoch 95/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0348\n",
            "Epoch 96/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0349\n",
            "Epoch 97/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0343\n",
            "Epoch 98/100\n",
            "93/93 [==============================] - 4s 44ms/step - loss: 0.0343\n",
            "Epoch 99/100\n",
            "93/93 [==============================] - 4s 44ms/step - loss: 0.0340\n",
            "Epoch 100/100\n",
            "93/93 [==============================] - 4s 43ms/step - loss: 0.0339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    model=solution_A5()\n",
        "    model.save(\"model_A5.h5\")\n"
      ],
      "metadata": {
        "id": "wAo24P-0sIGO"
      },
      "id": "wAo24P-0sIGO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}